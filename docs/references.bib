@article{Lauro1996,
  title = {Computational Statistics or Statistical Computing, Is That the Question?},
  author = {Lauro, Carlo},
  year = {1996},
  month = {nov},
  journal = {Computational Statistics \& Data Analysis},
  volume = {23},
  number = {1},
  pages = {191--193},
  issn = {01679473},
  doi = {10.1016/0167-9473(96)88920-1},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Lauro1996 - Computational statistics or statistical computing, is that the question.pdf},
}

@article{Breiman2001,
  title = {Statistical {{Modeling}}: {{The Two Cultures}} (with Comments and a Rejoinder by the Author)},
  shorttitle = {Statistical {{Modeling}}},
  author = {Breiman, Leo},
  year = {2001},
  month = {aug},
  journal = {Statistical Science},
  volume = {16},
  number = {3},
  pages = {199--231},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1009213726},
  abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Breiman2001 - Statistical Modeling.pdf;/Users/agarbuno/Zotero/storage/D9USILBT/1009213726.html},
}

@book{Kuhn2013,
  title = {Applied {{Predictive Modeling}}},
  author = {Kuhn, Max and Johnson, Kjell},
  year = {2013},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-6849-3},
  isbn = {978-1-4614-6848-6 978-1-4614-6849-3},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Kuhn2013 - Applied Predictive Modeling.pdf},
}
@book{James2021,
  title = {An {{Introduction}} to {{Statistical Learning}}: With {{Applications}} in {{R}}},
  shorttitle = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2021},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer US}},
  address = {{New York, NY}},
  langid = {english},
}
@article{Fourati2021,
  title = {A Survey of {{5G}} Network Systems: Challenges and Machine Learning Approaches},
  shorttitle = {A Survey of {{5G}} Network Systems},
  author = {Fourati, Hasna and Maaloul, Rihab and Chaari, Lamia},
  year = {2021},
  month = {feb},
  journal = {International Journal of Machine Learning and Cybernetics},
  volume = {12},
  number = {2},
  pages = {385--431},
  issn = {1868-808X},
  doi = {10.1007/s13042-020-01178-4},
  abstract = {5G cellular networks are expected to be the key infrastructure to deliver the emerging services. These services bring new requirements and challenges that obstruct the desired goal of forthcoming networks. Mobile operators are rethinking their network design to provide more flexible, dynamic, cost-effective and intelligent solutions. This paper starts with describing the background of the 5G wireless networks then we give a deep insight into a set of 5G challenges and research opportunities for machine learning (ML) techniques to manage these challenges. The first part of the paper is devoted to overview the fifth-generation of cellular networks, explaining its requirements as well as its key technologies, their challenges and its forthcoming architecture. The second part is devoted to present a basic overview of ML techniques that are nowadays applied to cellular networks. The last part discusses the most important related works which propose ML solutions in order to overcome 5G challenges.},
  langid = {english},
}
@book{Wood2017,
  title = {Generalized {{Additive Models}}},
  author = {Wood, Simon N},
  year = {2017},
  publisher = {{CRC Press}},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Wood2017 - Generalized Additive Models.pdf},
}
@book{Hastie2009c,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-84858-7},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Hastie2009 - The Elements of Statistical Learning.pdf},
}
@book{Zhang2021c,
  title = {Dive into {{Deep Learning}}},
  author = {Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  year = {2021},
  publisher = {{In print}},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Zhang2021 - Dive into Deep Learning.pdf},
}
@article{Halmos1970,
  title = {How to Write Mathematics},
  author = {Halmos, PR},
  year = {1970},
  journal = {L'Enseignement Math\'ematique},
  volume = {16},
}
@book{Breiman2017,
  title = {Classification {{And Regression Trees}}},
  author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
  year = {2017},
  month = {oct},
  publisher = {{Routledge}},
  address = {{New York}},
  doi = {10.1201/9781315139470},
  abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
  isbn = {978-1-315-13947-0},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Breiman2017 - Classification And Regression Trees.pdf},
}
@book{Ripley1996,
  title = {Pattern {{Recognition}} and {{Neural Networks}}},
  author = {Ripley, Brian D.},
  year = {1996},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511812651},
  abstract = {This 1996 book is a reliable account of the statistical framework for pattern recognition and machine learning. With unparalleled coverage and a wealth of case-studies this book gives valuable insight into both the theory and the enormously diverse applications (which can be found in remote sensing, astrophysics, engineering and medicine, for example). So that readers can develop their skills and understanding, many of the real data sets used in the book are available from the author's website: www.stats.ox.ac.uk/\textasciitilde ripley/PRbook/. For the same reason, many examples are included to illustrate real problems in pattern recognition. Unifying principles are highlighted, and the author gives an overview of the state of the subject, making the book valuable to experienced researchers in statistics, machine learning/artificial intelligence and engineering. The clear writing style means that the book is also a superb introduction for non-specialists.},
  isbn = {978-0-521-71770-0},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Ripley1996 - Pattern Recognition and Neural Networks.pdf;/Users/agarbuno/Zotero/storage/ED8TGF3R/4E038249C9BAA06C8F4EE6F044D09C5C.html},
}
@book{Schapire2001,
  title = {The Boosting Approach to Machine Learning: {{An}} Overview},
  author = {Schapire, R.},
  year = {2001},
  keywords = {classification,Machine Learning},
}
@book{Shalev-Shwartz2014,
  title = {Understanding Machine Learning: From Theory to Algorithms},
  shorttitle = {Understanding Machine Learning},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  publisher = {{Cambridge University Press}},
  address = {{New York, NY, USA}},
  abstract = {"Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering"--},
  isbn = {978-1-107-05713-5},
  langid = {english},
  lccn = {Q325.5 .S475 2014},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Shalev-Shwartz2014 - Understanding machine learning.pdf},
}
@article{Abeykoon2022,
  title = {Stochastic Gradient Descent-Based Support Vector Machines Training Optimization on {{Big Data}} and {{HPC}} Frameworks},
  author = {Abeykoon, Vibhatha and Fox, Geoffrey and Kim, Minje and Ekanayake, Saliya and Kamburugamuve, Supun and Govindarajan, Kannan and Wickramasinghe, Pulasthi and Perera, Niranda and Widanage, Chathura and Uyar, Ahmet and Gunduz, Gurhan and Akkas, Selahatin},
  year = {2022},
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {34},
  number = {8},
  pages = {e6292},
  issn = {1532-0634},
  doi = {10.1002/cpe.6292},
  abstract = {Support vector machines (SVM) is a widely used machine learning algorithm. With the increasing amount of research data nowadays, understanding how to do efficient training is more important than ever. This article discusses the performance optimizations and benchmarks related to providing high-performance support for SVM training. In this research, we have focused on a highly scalable gradient descent-based approach to implementing the core SVM algorithm. In providing a scalable solution, we have designed optimized high-performance computing and dataflow-oriented SVM implementations. A high-performance computing approach means the algorithm is implemented with the bulk synchronous parallel (BSP) model. In addition, we analyzed the language level optimizations and math kernel optimizations on a prominent HPC modeling programming language (C++) and dataflow modeling programming language (Java). In the experiments, we compared the performance of classic HPC models, classic dataflow models, and hybrid models designed on classic HPC and dataflow programming models. Our research illustrates a scientific approach in designing the SVM algorithm at scale in classic HPC, dataflow, and hybrid systems.},
  langid = {english},
  keywords = {dataflow,high-performance computing,hybrid systems,machine learning,SVM},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.6292},
  file = {/Users/agarbuno/Zotero/storage/HDQN6KA5/cpe.html},
}
@book{Zhang2021c,
  title = {Dive into {{Deep Learning}}},
  author = {Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  year = {2021},
  publisher = {{In print}},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Book/Zhang2021 - Dive into Deep Learning.pdf},
}
@article{Rosenblatt1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471},
  doi = {10.1037/h0042519},
  abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Brain,Cognition,Memory,Nervous System},
  file = {/Users/agarbuno/Zotero/storage/D32WSAWP/1959-09865-001.html},
}
@article{Rumelhart1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = {oct},
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Nature Publishing Group},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Rumelhart1986 - Learning representations by back-propagating errors.pdf;/Users/agarbuno/Zotero/storage/BXGXTLLS/323533a0.html},
}
@article{Prisacariu2012,
  title = {{{PWP3D}}: {{Real-Time Segmentation}} and {{Tracking}} of {{3D Objects}}},
  shorttitle = {{{PWP3D}}},
  author = {Prisacariu, Victor A. and Reid, Ian D.},
  year = {2012},
  month = {jul},
  journal = {International Journal of Computer Vision},
  volume = {98},
  number = {3},
  pages = {335--354},
  issn = {1573-1405},
  doi = {10.1007/s11263-011-0514-3},
  abstract = {We formulate a probabilistic framework for simultaneous region-based 2D segmentation and 2D to 3D pose tracking, using a known 3D model. Given such a model, we aim to maximise the discrimination between statistical foreground and background appearance models, via direct optimisation of the 3D pose parameters. The foreground region is delineated by the zero-level-set of a signed distance embedding function, and we define an energy over this region and its immediate background surroundings based on pixel-wise posterior membership probabilities (as opposed to likelihoods). We derive the differentials of this energy with respect to the pose parameters of the 3D object, meaning we can conduct a search for the correct pose using standard gradient-based non-linear minimisation techniques. We propose novel enhancements at the pixel level based on temporal consistency and improved online appearance model adaptation. Furthermore, straightforward extensions of our method lead to multi-camera and multi-object tracking as part of the same framework. The parallel nature of much of the processing in our algorithm means it is amenable to GPU acceleration, and we give details of our real-time implementation, which we use to generate experimental results on both real and artificial video sequences, with a number of 3D models. These experiments demonstrate the benefit of using pixel-wise posteriors rather than likelihoods, and showcase the qualities, such as robustness to occlusions and motion blur (and also some failure modes), of our tracker.},
  langid = {english},
  keywords = {3D tracking,Cuda,GPGPU,Level set,Pose recovery,Real time,Region based,Segmentation,Tracking},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Prisacariu2012 - PWP3D.pdf},
}
@article{Salakhutdinov2009b,
  title = {Semantic Hashing},
  author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  year = {2009},
  month = {jul},
  journal = {International Journal of Approximate Reasoning},
  volume = {50},
  number = {7},
  pages = {969--978},
  issn = {0888613X},
  doi = {10.1016/j.ijar.2008.11.006},
  abstract = {We show how to learn a deep graphical model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use a small number of binary variables (e.g. 32), the graphical model performs ``semantic hashing'': Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method. By using semantic hashing to filter the documents given to TF-IDF, we achieve higher accuracy than applying TF-IDF to the entire document set.},
  langid = {english},
  file = {/Volumes/GoogleDrive/My Drive/bibliography/Journal Article/Salakhutdinov2009 - Semantic hashing.pdf},
}
@book{Kuhn2022,
  title={Tidy Modeling with R},
  author={Kuhn, Max and Silge, Julia},
  year={2022},
  publisher={"O'Reilly Media, Inc."}
}