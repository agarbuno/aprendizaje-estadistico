\section{Minimización de pérdida regularizada (RLM)}
RLM = pérdida empírica + función de regularización.
\\

El objetivo: $\underset{w \in H}{\mbox{argmin}} L_s(w)+R(w)$. Queremos, pues, encontrar un balance entre modelos simples y soluciones con error empírico pequeño.
$R()$ normalmente se escoje con conocimiento de dominio, y las opciones clásicas son $R(w) = \lambda*\|w\|^2_2$ ó $R(w) = \lambda*\|w\|^2_1$
\\

Regularización de Tikhonov (se usa para problemas inversos): supongamos que tenemos un problema de regresión de la forma $\frac{1}{2m}\|x_w-y\|^2 + \frac{\lambda}{2}\|w\|^2$, el cual buscamos minimizar.
Ya hemos visto que $\frac{1}{2m}\|x_w-y\|^2$ tiene un mínimo. La ventaja de $\frac{\lambda}{2}\|w\|^2$ es que no afecta mucho ese mínimo y, además, la solución de $\Delta_w(L_s(w)+R(w)=0$ es $(\lambda_m*\mathbbm{1}_{pxp})w = x^Ty*2w = (x^Tx +\lambda_m*\mathbbm{1}_{pxp})^{-1}x^Ty $.
Por ejemplo, si $P(w) = w_0+w_1x+\dots+w_px^p$ y $\lambda >> 0 $, esperaríamos que el modelo fuera parsimonioso y que los términos mayores se fueran eliminando.
\\

Verificaremos que $R(w)$ estabiliza y permite el sobreajuste. En particular, veremos un resultados en que 
\begin{center}
    $E_S(L_D(A(S))) \leq \underset{w \in H}{\min} L_D(w) + \varepsilon$
\end{center}

Otra forma de pensar el problema es: $\underset{w \in H}{\min} L_D(w)$ sujeto a $\|w\|^2_2 \leq \theta$

