\documentclass[a4paper,11pt,oneside]{book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subcaption} 
\usepackage{hyperref}
\usepackage[letterpaper, margin=1.8cm]{geometry}
\usepackage{mathtools}
\usepackage{url}
\usepackage[spanish]{babel} % español
\usepackage[utf8]{inputenc} % acentos sin código
\usepackage[table,xcdraw]{xcolor}
\usepackage{sectsty}
\definecolor{verdeITAM}{RGB}{0, 108, 84}
\chapterfont{\color{verdeITAM}}
\sectionfont{\color{verdeITAM}}
\subsectionfont{\color{verdeITAM}}
\subsubsectionfont{\color{verdeITAM}}
\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{multirow}  
\usepackage{import}
\usepackage{makeidx}
\usepackage{bbm}
\newtheorem{theorem}{Teorema}
\newtheorem{definition}{Definición}
\newtheorem{lemma}{Lema}
\makeindex

\pagestyle{empty}

\begin{document}

\frontmatter
\import{./}{Titulo.tex}

\clearpage
\thispagestyle{empty}

\tableofcontents

\mainmatter
\chapter{Regularización y estabilidad}
Hoy veremos que CLA y CSA son realmente logrables/aprendibles. Hay instancias de esos dos que son uniformes y por lo tanto aprendibles, con la regla más sencilla: ERM.
El único problema es que, en general, no es cierto que la mayoría de las CLA o CSA seana aprendibles, entonces necesitamos un nuevo paradigma que nos garantice ello: minimización de pérdida regularizada.

Idea: tomamos una función de pérdida y sumamos algo que regularice.

Nota: regularización nos sirve para medir la complejidad de la hipótesis y actúa como un estabilizador).

\import{Secciones/}{1-1.tex}
\import{Secciones/}{1-2.tex}
\import{Secciones/}{1-3.tex}
\import{Secciones/}{1-3-1.tex}
\import{Secciones/}{1-4.tex}



\end{document}

