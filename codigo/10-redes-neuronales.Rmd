---
title: "Redes Neuronales"
author: Alfredo Garbuno
---

```{r setup, include = FALSE}
library(tidyverse)
library(rsample)
library(tidymodels)

library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)
options(digits=2)

library(patchwork)
library(scales)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE, 
                      fig.align = 'center', fig.width = 5, fig.height=3, cache = TRUE, 
                      out.width = "99%")
comma <- function(x) format(x, digits = 2, big.mark = ",")
theme_set(theme_linedraw())
color.blues <- c(NA,"#BDD7E7", "#6BAED6", "#3182BD", "#08519C", "#074789", "#063e77", "#053464")
color.itam  <- c("#00362b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f")


sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
sin_leyenda <- theme(legend.position = "none")
sin_ejes <- theme(axis.ticks = element_blank(), 
        axis.text = element_blank())
```

# Instalación de Keras

```{r}
library(keras)
# install_keras(method = "conda",
#               conda = "/Users/agarbuno/anaconda3/condabin/conda",
#               envname = "statistical-learning")

library(reticulate)
use_python("/Users/agarbuno/anaconda3/envs/statistical-learning/bin/python")
use_condaenv("/Users/agarbuno/anaconda3/envs/statistical-learning/")
```

# Ejemplo XOR

```{r}
# para reproducibilidad:
tensorflow::tf$random$set_seed(1123)
```

```{r}
h <- function(x){
    exp(x)/(1 + exp(x))
}

p <- function(x1, x2){
  h(-5 + 10*x1 + 10*x2 - 30*x1*x2)
}
dat <- expand.grid(x1 = seq(0, 1, 0.05), x2 = seq(0, 1, 0.05))
dat <- dat %>% mutate(p = p(x1, x2))
gtruth <- ggplot(dat, aes(x=x1, y=x2)) + geom_tile(aes(fill=p))
gtruth
```

```{r}
set.seed(322)
n <- 1000
dat_ent <- tibble(x1 = runif(n,0,1), x2 = runif(n, 0, 1)) %>%
  mutate(p = p(x1, x2)) %>%
  mutate(y = rbinom(n, 1, p))
mod_1 <- glm(y ~ x1 + x2, data = dat_ent, family = 'binomial')
mod_1
```

```{r}
table(predict(mod_1) > 0.5, dat_ent$y)
```

```{r}
mod_2 <- glm(y ~ x1 + x2 + x1:x2, data = dat_ent, family = 'binomial')
mod_2
```

```{r}
table(predict(mod_2) > 0.5, dat_ent$y)
```

```{r}
reg <- regularizer_l2(0.0001)
mod_inter <- keras_model_sequential()
mod_inter %>% 
    layer_dense(units = 4, 
              activation = "relu",
              name = "capa_intermedia", 
              kernel_initializer = initializer_random_normal(stddev = 0.01, seed = 101),
              kernel_regularizer = reg,
              input_shape = c(2)) %>%
    layer_dense(units = 1, 
              activation = "sigmoid",
              name = "capa_salida", 
              kernel_regularizer = reg,
              kernel_initializer = initializer_random_normal(stddev = 0.01, seed = 201),
              input_shape = c(2))  
```

```{r}

features <- tibble(x_1 = c(0,0,1,1), x_2 = c(0,1,0,1)) %>% as.matrix
labels   <- tibble(y   = c(0,1,1,0)) %>% as.matrix

mod_inter %>% compile(loss = "binary_crossentropy", 
  optimizer = optimizer_sgd(lr = 1.0))
historia <- mod_inter %>% 
    fit(dat_ent %>% select(x1, x2) %>% as.matrix, dat_ent$y, 
      batch_size = nrow(dat_ent), epochs = 10000, verbose = 0)
```

```{r}
preds <- predict(mod_inter, dat %>% select(x1, x2) %>% as.matrix)
dat <- dat %>% mutate(p_red = preds)
gpreds <- ggplot(dat, aes(x=x1, y=x2)) + geom_tile(aes(fill = p_red))

gtruth + gpreds
```

```{r}

beta <- get_weights(mod_inter)
beta
```

```{r}
mat_entrada <- tibble(x_1 = c(0,0,1,1), x_2 = c(0,1,0,1)) %>% as.matrix
capa_1 <- keras_model(inputs = mod_inter$input,
    outputs = get_layer(mod_inter, "capa_intermedia")$output)
predict(capa_1, mat_entrada) %>% round(2)
```

# Ejemplo Diabetes

```{r}
diabetes_ent <- MASS::Pima.tr
diabetes_pr <- MASS::Pima.te
x_ent <- diabetes_ent %>% select(-type) %>% as.matrix
x_ent_s <- scale(x_ent)
x_valid <- diabetes_pr %>% select(-type) %>% as.matrix 
x_valid_s <- x_valid %>%
  scale(center = attr(x_ent_s, 'scaled:center'), 
        scale = attr(x_ent_s,  'scaled:scale'))
y_ent <- as.numeric(diabetes_ent$type == 'Yes')
y_valid <- as.numeric(diabetes_pr$type == 'Yes')
```

```{r}
modelo_tc <- keras_model_sequential() 
# no es necesario asignar a nuevo objeto, modelo_tc es modificado al agregar capas
modelo_tc %>% 
  layer_dense(units = 10, activation = 'sigmoid', 
              kernel_regularizer = regularizer_l2(l = 1e-3), 
              kernel_initializer = initializer_random_uniform(minval = -0.5, maxval = 0.5),
              input_shape=7) %>%
  layer_dense(units = 10, activation = 'sigmoid', 
              kernel_regularizer = regularizer_l2(l = 1e-3), 
              kernel_initializer = initializer_random_uniform(minval = -0.5, maxval = 0.5)) %>%
  layer_dense(units = 1, activation = 'sigmoid',
              kernel_regularizer = regularizer_l2(l = 1e-3),
              kernel_initializer = initializer_random_uniform(minval = -0.5, maxval = 0.5)
)
```

```{r}
modelo_tc %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_sgd(lr = 0.3),
  metrics = c('accuracy','binary_crossentropy'))
```


```{r}
iteraciones <- modelo_tc %>% fit(
  x_ent_s, y_ent, 
  #batch size mismo que nrow(x_ent_s) es descenso en grad.
  epochs = 1000, batch_size = nrow(x_ent_s), 
  verbose = 0,
  validation_data = list(x_valid_s, y_valid)
)
```

```{r}
score <- modelo_tc %>% evaluate(x_valid_s, y_valid)
score
```

```{r}
tab_confusion <- table(modelo_tc %>% predict_classes(x_valid_s),y_valid) 
tab_confusion
```

```{r}
prop.table(tab_confusion, 2)
```

```{r}
df_iteraciones <- as_tibble(iteraciones)
ggplot(df_iteraciones, aes(x=epoch, y=value, colour=data, group=data)) + 
  geom_line() + geom_point() + facet_wrap(~metric, ncol=1, scales = 'free')
```

```{r}
iteraciones <- modelo_tc %>% fit(
  x_ent_s, y_ent, 
  #batch size mismo que nrow(x_ent_s) es descenso en grad.
  epochs = 500, batch_size = nrow(x_ent_s), 
  verbose = 0,
  callbacks = callback_tensorboard("logs/diabetes/run_1"),
  validation_data = list(x_valid_s, y_valid)
)
```

```{r}
tensorboard("logs/diabetes/")
```

# Selección de hiper-parámetros

```{r}
data(bivariate)
nrow(bivariate_train)
nrow(bivariate_val)

```

```{r}
ggplot(bivariate_train, aes(x = A, y = B, col = Class)) + 
  geom_point(alpha = .2)
```

```{r}
biv_rec <- 
  recipe(Class ~ ., data = bivariate_train) %>%
  step_BoxCox(all_predictors())%>%
  step_normalize(all_predictors()) %>%
  prep(training = bivariate_train, retain = TRUE)

# We will bake(new_data = NULL) to get the processed training set back

# For validation:
val_normalized <- bake(biv_rec, new_data = bivariate_val, all_predictors())
# For testing when we arrive at a final model: 
test_normalized <- bake(biv_rec, new_data = bivariate_test, all_predictors())
```

```{r}
set.seed(57974)
nnet_fit <-
  mlp(epochs = 1000, 
      hidden_units = tune(), 
      activation = tune()) %>%
  set_mode("classification") %>% 
  # Also set engine-specific `verbose` argument to prevent logging the results: 
  set_engine("keras", verbose = 0) %>%
  fit(Class ~ ., data = bake(biv_rec, new_data = NULL))

nnet_fit
```

